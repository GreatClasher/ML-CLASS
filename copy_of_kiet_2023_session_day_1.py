# -*- coding: utf-8 -*-
"""Copy of KIET 2023 Session Day 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TCsJMuudyoJIrFHuwpmJ7zmNI5hU0zzP
"""

import pandas as pd
import numpy as np

data = pd.read_csv("data.csv")

data

data.iloc[4:567,:]

training_data_len = int(0.7*len(data))
testing_data_len = int(0.3*len(data))

training_data_len

testing_data_len

neg_class_data = data[data["diagnosis"] == 'B']

pos_class_data = data[data["diagnosis"] == 'M']

neg_class_data.shape

pos_class_data.shape

train_neg_class_data = neg_class_data.iloc[0:training_data_len//2,:]
train_pos_class_data = pos_class_data.iloc[0:training_data_len//2,:]

test_neg_class_data = neg_class_data.iloc[training_data_len//2:,:]
test_pos_class_data = pos_class_data.iloc[training_data_len//2:,:]

train_neg_class_data.shape

train_pos_class_data.shape

test_neg_class_data.shape

test_pos_class_data.shape

training_data = pd.concat([train_neg_class_data,train_pos_class_data])

training_data.shape

testing_data = pd.concat([test_neg_class_data,test_pos_class_data])

testing_data.shape

data

training_data_copy = training_data.drop([data.columns[32]],axis=1)

data.columns[32]

training_data.shape

training_data_copy.shape

training_data.drop([data.columns[32]],axis=1,inplace=True)

training_data.shape

training_data

import matplotlib.pyplot as plt

training_data.columns

plt.scatter(training_data[data.columns[2]],training_data[data.columns[3]])

training_data.corr()

training_data[data.columns[1]].replace(to_replace=['B','M'],value=[0,1],inplace=True)

training_data

testing_data[data.columns[1]].replace(to_replace=['B','M'],value=[0,1],inplace=True)

testing_data

corr_matrix = training_data.corr()

corr_matrix

l1 = [0.5,0.6,0.7,0.8]
D= dict(corr_matrix[data.columns[1]] > 0.7)

# from sklearn.model_selection import GridSearchCV
# import warnings
# warnings.filterwarnings('ignore')
# # parameter grid
# parameters = {
#     'penalty' : ['l1','l2'], 
#     'C'       : np.logspace(-3,3,7),
#     'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],
# }

D.keys()

filtered_columns = list()

for k in D.keys():

  if D[k] == True:

    filtered_columns.append(k)

len(filtered_columns)

filtered_training_data = training_data[filtered_columns]

filtered_training_data

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

answers = filtered_training_data[data.columns[1]]

input_features = filtered_training_data.iloc[:,1:]

input_features

naive_bayes_algo = GaussianNB()

naive_bayes_algo.fit(X=input_features,y=answers)

filtered_testing_data = testing_data[filtered_columns]

filtered_testing_data

testing_answers = filtered_testing_data[data.columns[1]]

testing_input_features = filtered_testing_data.iloc[:,1:]

exam_answers = naive_bayes_algo.predict(testing_input_features)

exam_answers

from sklearn.metrics import classification_report

# logreg = LogisticRegression()
# clf = GridSearchCV(logreg,                    # model
#                    param_grid = parameters,   # hyperparameters
#                    scoring='accuracy',        # metric for scoring
#                    cv=10)

# print("Tuned Hyperparameters :", clf.best_params_)
# print("Accuracy :",clf.best_score_)

print(classification_report(y_true=testing_answers,y_pred=exam_answers))

# logistic_regression_algo = LogisticRegression(tol=0.000001,max_iter=10000)

# logistic_regression_algo.fit(X=input_features,y=answers)

# exam_answers = logistic_regression_algo.predict(testing_input_features)

# exam_answers

# print(classification_report(y_true=testing_answers,y_pred=exam_answers))

